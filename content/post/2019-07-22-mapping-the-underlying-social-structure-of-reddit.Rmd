---
title: Mapping the Underlying Social Structure of Reddit
author: Andrew Carr
date: '2019-07-22'
slug: mapping-the-underlying-social-structure-of-reddit
categories: []
tags: []
---

Since it was founded in 2005, Reddit has developed into a popular place for sharing opinions and ideas, rating web content, and aggregating news on the internet.  Reddit is organized into thousands of user-made communities, called subreddits, which cover a broad range of subjects, including politics, sports, technology, personal hobbies, and self-improvement, to name a few. Given that Reddit is structured in this way, it is natural to think of Reddit as a population of users organized into many overlapping communities (subreddits).  In other words, it makes sense to conceptualize Reddit as having an underlying social structure.  Uncovering this structure may provide insights into the social organization of internet culture more generally.  

My goal in this post is to map the social structure of Reddit by measuring the proximity of Reddit communities to each other.  I'm operationalizing proximity between subreddits as the number of submissions on these subreddits that come from the same user in a given time period.  For example, if a user posts something to subreddit A and then a few days later posts something else to subreddit B, subreddits A and B are linked by this user.  Subreddits with more users in common are closer together.  The idea that networks between groups are formed by the people these groups have in common is an old one in sociology (Breiger 1974); more recently, it has served as a conceptual basis for producing networks from internet data (https://www-cs.stanford.edu/~jure/pubs/cesna-icdm13.pdf, https://cs.stanford.edu/people/jure/pubs/agmfit-icdm12.pdf).  To my knowledge, this idea hasn't been used to examine the social structure of communities on Reddit, although other methods have been used to analyze the community structure of Reddit (Olson and Neal 2015).  

# Data 

The data I'm going to use for this post come from a marvelous online repository of subreddit submissions, comments, and other content that is generously hosted by data scientist Jason Baumgartner.  Although I'm only making use of the submissions section of Baumgartner's repository, I've downloaded a lot of his data.  If you plan to download a lot of data from this repo, I implore you to donate a bit of money to keep Baumgartner's database up and running (donate here - https://pushshift.io/donations/).  Hosting this data is not free!    

Here's the link to Baumgarter's Reddit submissions data - http://files.pushshift.io/reddit/submissions/.  Each of these files has all Reddit submissions for a given month between June 2005 and May 2019.  Files are JSON objects stored in various compression formats that range between .017Mb and 5.77Gb in size, a testament to how much Reddit has grown over the years.  Let's get our feet wet with this data by downloading something in the middle of this range - a 710Mb file for all Reddit submissions in May 2013.  The file is called RS_2013-05.bz2. It will take a minute or to download.  You can double-click this file to unzip it, or if you prefer to work in the Terminal use the following command: `bzip2 -d RS_2013-05.bz2`.  This will take a couple of minutes to unzip.  Make sure you have enough room to store the unzipped file on your computer - it's 4.51Gb.  
Once we have unzipped this file, load the readr, jsonlite, and dplyr packages.  Use the read_lines function from readr and set the n_max parameter to 1 to read the first line from the file.  Pipe this through fromJSON and names to get a list of the variable names contained in the data.

```{r, include = FALSE}
library(readr)
library(jsonlite)
library(dplyr)
```

```{r}
read_lines("RS_2013-05", n_max = 1) %>% fromJSON() %>% names
```

The data contain 35 bits of information for each submission.  For this project, I'm only interested in three things: the user name associated with the submission (author), the subreddit to which a submission has been posted (subreddit), and the time of submission (created_utc).  Everything else is extraneous information.  

If we could figure out a way to extract these three pieces of information from each line of JSON we could greatly reduce the size of our data, which would allow us to store multiple months worth of information on our local machine.  Jq is a command-line JSON processor that makes this possible.  

I'm going to walk you through installing jq on a Mac (sorry Windows users!).  First, you need to make sure you have Home Brew installed (https://brew.sh/).  Home Brew is a package manager for the Mac that works in the Terminal.  The following instructions assume you have Home Brew installed.  To begin, open the Terminal (press Cmd+Space, type Terminal, press Enter).  To install jq, type `brew install jq`.  Next, we'll extract the variables we want from RS_2015-03 and save the result as a .csv file, all in one file of code.  To select variables with jq, we want to list the JSON field names that we want like this: `[.author, .created_utc, .subreddit]`.  We want to return these as raw output (`-r`) and we want this outtput to be rendered as a csv file (`@csv`).  Taken together, the magic command that accomplishes what we want is this:

`jq -r '[.author, .created_utc, .subreddit] | @csv' RS_2013-05  > parsed_json_to_csv_2013_05`

Make sure the Terminal directory is set to wherever RS_2013-05 is located before running this command.  I am by no means a master of jq syntax, but what I think this command says is to pull "author", "created_utc", and "subreddit" from each line of the JSON file and render the output as a .csv file.  The pipe operator ("|") takes output to its right and applies the command to its left to this output.  The resultant file will be saved as "parsed_json_to_csv_2013_05".  The jq command is pulling these fields from millions of lines of JSON (every Reddit submission from 03-2015), so this process can take a few minutes.  In case you're new to working in the Terminal, if there's a blank line at the bottom of the Terminal window, that means the process is still running.  When the directory followed by "$" reappears, the process is complete.  The file parsed_json_to_csv_2013_05 should now appear in whatever directory you're in in the Terminal.  This file is about 118Mb, much smaller than 4.5Gb.

# Analysis 






