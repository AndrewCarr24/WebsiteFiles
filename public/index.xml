<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data Science Diarist</title>
    <link>https://datadiarist.github.io/</link>
    <description>Recent content on Data Science Diarist</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 22 Jul 2019 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://datadiarist.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Mapping the Underlying Social Structure of Reddit</title>
      <link>https://datadiarist.github.io/post/mapping-the-underlying-social-structure-of-reddit/</link>
      <pubDate>Mon, 22 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://datadiarist.github.io/post/mapping-the-underlying-social-structure-of-reddit/</guid>
      <description>


&lt;p&gt;Since it was founded in 2005, Reddit has developed into a popular place for sharing opinions and ideas, rating web content, and aggregating news on the internet. Reddit is organized into thousands of user-made communities, called subreddits, which cover a broad range of subjects, including politics, sports, technology, personal hobbies, and self-improvement, to name a few. Given that Reddit is structured in this way, it is natural to think of Reddit as a population of users organized into many overlapping communities (subreddits). In other words, it makes sense to conceptualize Reddit as having an underlying social structure. Uncovering this structure may provide insights into the social organization of internet culture more generally.&lt;/p&gt;
&lt;p&gt;My goal in this post is to map the social structure of Reddit by measuring the proximity of Reddit communities to each other. I’m operationalizing proximity between subreddits as the number of submissions on these subreddits that come from the same user in a given time period. For example, if a user posts something to subreddit A and then a few days later posts something else to subreddit B, subreddits A and B are linked by this user. Subreddits with more users in common are closer together. The idea that networks between groups are formed by the people these groups have in common is an old one in sociology (Breiger 1974); more recently, it has served as a conceptual basis for producing networks from internet data (&lt;a href=&#34;https://www-cs.stanford.edu/~jure/pubs/cesna-icdm13.pdf&#34; class=&#34;uri&#34;&gt;https://www-cs.stanford.edu/~jure/pubs/cesna-icdm13.pdf&lt;/a&gt;, &lt;a href=&#34;https://cs.stanford.edu/people/jure/pubs/agmfit-icdm12.pdf&#34; class=&#34;uri&#34;&gt;https://cs.stanford.edu/people/jure/pubs/agmfit-icdm12.pdf&lt;/a&gt;). To my knowledge, this idea hasn’t been used to examine the social structure of communities on Reddit, although other methods have been used to analyze the community structure of Reddit (Olson and Neal 2015).&lt;/p&gt;
&lt;div id=&#34;data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data&lt;/h1&gt;
&lt;p&gt;The data I’m going to use for this post come from a marvelous online repository of subreddit submissions, comments, and other content that is generously hosted by data scientist Jason Baumgartner. Although I’m only making use of the submissions section of Baumgartner’s repository, I’ve downloaded a lot of his data. If you plan to download a lot of data from this repo, I implore you to donate a bit of money to keep Baumgartner’s database up and running (donate here - &lt;a href=&#34;https://pushshift.io/donations/&#34; class=&#34;uri&#34;&gt;https://pushshift.io/donations/&lt;/a&gt;). Hosting this data is not free!&lt;/p&gt;
&lt;p&gt;Here’s the link to Baumgarter’s Reddit submissions data - &lt;a href=&#34;http://files.pushshift.io/reddit/submissions/&#34; class=&#34;uri&#34;&gt;http://files.pushshift.io/reddit/submissions/&lt;/a&gt;. Each of these files has all Reddit submissions for a given month between June 2005 and May 2019. Files are JSON objects stored in various compression formats that range between .017Mb and 5.77Gb in size, a testament to how much Reddit has grown over the years. Let’s get our feet wet with this data by downloading something in the middle of this range - a 710Mb file for all Reddit submissions in May 2013. The file is called RS_2013-05.bz2. It will take a minute or to download. You can double-click this file to unzip it, or if you prefer to work in the Terminal use the following command: &lt;code&gt;bzip2 -d RS_2013-05.bz2&lt;/code&gt;. This will take a couple of minutes to unzip. Make sure you have enough room to store the unzipped file on your computer - it’s 4.51Gb.&lt;br /&gt;
Once we have unzipped this file, load the readr, jsonlite, and dplyr packages. Use the read_lines function from readr and set the n_max parameter to 1 to read the first line from the file. Pipe this through fromJSON and names to get a list of the variable names contained in the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;read_lines(&amp;quot;RS_2013-05&amp;quot;, n_max = 1) %&amp;gt;% fromJSON() %&amp;gt;% names&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;edited&amp;quot;                 &amp;quot;title&amp;quot;                 
##  [3] &amp;quot;thumbnail&amp;quot;              &amp;quot;retrieved_on&amp;quot;          
##  [5] &amp;quot;mod_reports&amp;quot;            &amp;quot;selftext_html&amp;quot;         
##  [7] &amp;quot;link_flair_css_class&amp;quot;   &amp;quot;downs&amp;quot;                 
##  [9] &amp;quot;over_18&amp;quot;                &amp;quot;secure_media&amp;quot;          
## [11] &amp;quot;url&amp;quot;                    &amp;quot;author_flair_css_class&amp;quot;
## [13] &amp;quot;media&amp;quot;                  &amp;quot;subreddit&amp;quot;             
## [15] &amp;quot;author&amp;quot;                 &amp;quot;user_reports&amp;quot;          
## [17] &amp;quot;domain&amp;quot;                 &amp;quot;created_utc&amp;quot;           
## [19] &amp;quot;stickied&amp;quot;               &amp;quot;secure_media_embed&amp;quot;    
## [21] &amp;quot;media_embed&amp;quot;            &amp;quot;ups&amp;quot;                   
## [23] &amp;quot;distinguished&amp;quot;          &amp;quot;selftext&amp;quot;              
## [25] &amp;quot;num_comments&amp;quot;           &amp;quot;banned_by&amp;quot;             
## [27] &amp;quot;score&amp;quot;                  &amp;quot;report_reasons&amp;quot;        
## [29] &amp;quot;id&amp;quot;                     &amp;quot;gilded&amp;quot;                
## [31] &amp;quot;is_self&amp;quot;                &amp;quot;subreddit_id&amp;quot;          
## [33] &amp;quot;link_flair_text&amp;quot;        &amp;quot;permalink&amp;quot;             
## [35] &amp;quot;author_flair_text&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data contain 35 bits of information for each submission. For this project, I’m only interested in three things: the user name associated with the submission (author), the subreddit to which a submission has been posted (subreddit), and the time of submission (created_utc). Everything else is extraneous information.&lt;/p&gt;
&lt;p&gt;If we could figure out a way to extract these three pieces of information from each line of JSON we could greatly reduce the size of our data, which would allow us to store multiple months worth of information on our local machine. Jq is a command-line JSON processor that makes this possible.&lt;/p&gt;
&lt;p&gt;I’m going to walk you through installing jq on a Mac (sorry Windows users!). First, you need to make sure you have Home Brew installed (&lt;a href=&#34;https://brew.sh/&#34; class=&#34;uri&#34;&gt;https://brew.sh/&lt;/a&gt;). Home Brew is a package manager for the Mac that works in the Terminal. The following instructions assume you have Home Brew installed. To begin, open the Terminal (press Cmd+Space, type Terminal, press Enter). To install jq, type &lt;code&gt;brew install jq&lt;/code&gt;. Next, we’ll extract the variables we want from RS_2015-03 and save the result as a .csv file, all in one file of code. To select variables with jq, we want to list the JSON field names that we want like this: &lt;code&gt;[.author, .created_utc, .subreddit]&lt;/code&gt;. We want to return these as raw output (&lt;code&gt;-r&lt;/code&gt;) and we want this outtput to be rendered as a csv file (&lt;code&gt;@csv&lt;/code&gt;). Taken together, the magic command that accomplishes what we want is this:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;jq -r &#39;[.author, .created_utc, .subreddit] | @csv&#39; RS_2013-05  &amp;gt; parsed_json_to_csv_2013_05&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Make sure the Terminal directory is set to wherever RS_2013-05 is located before running this command. I am by no means a master of jq syntax, but what I think this command says is to pull “author”, “created_utc”, and “subreddit” from each line of the JSON file and render the output as a .csv file. The pipe operator (“|”) takes output to its right and applies the command to its left to this output. The resultant file will be saved as “parsed_json_to_csv_2013_05”. The jq command is pulling these fields from millions of lines of JSON (every Reddit submission from 03-2015), so this process can take a few minutes. In case you’re new to working in the Terminal, if there’s a blank line at the bottom of the Terminal window, that means the process is still running. When the directory followed by “$” reappears, the process is complete. The file parsed_json_to_csv_2013_05 should now appear in whatever directory you’re in in the Terminal. This file is about 118Mb, much smaller than 4.5Gb.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;analysis&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Analysis&lt;/h1&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Building a Recommendation System with Beer Data</title>
      <link>https://datadiarist.github.io/post/building-a-recommendation-system-with-beer-data/</link>
      <pubDate>Tue, 16 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://datadiarist.github.io/post/building-a-recommendation-system-with-beer-data/</guid>
      <description>


&lt;p&gt;Beer culture in the United States has changed dramatically in the past decade or so. One reflection of this is that the number of small-scale breweries has increased in every state in the country. This trend is also reflected in the development of a vibrant community of people who rate, review, and share information about beers online. Websites like BeerAdvocate, RateBeer, and Untappd provide forums for beer drinkers to discuss their favorite beers with each other. Surprisingly, these sites differ from product marketplaces such as Amazon in that they do not recommend new products to users based on users’ previous evaluations.&lt;/p&gt;
&lt;p&gt;Many of these sites have accumulated enough information about users’ beer preferences to train systems for recommending beers based on users’ tastes. This inspired me to create my own recommendation engine by scraping the data on some of these sites. While some beer sites prohibit web scraping outright, others only disallow scraping their data for commercial use. Others place no restrictions on scraping at all. For this project, I only scraped sites that did not proscribe scraping in a robots.txt file. Ethical (and legal) web scraping has been made easier with the recent development of the polite package, which you should check out (&lt;a href=&#34;https://github.com/dmi3kno/polite&#34; class=&#34;uri&#34;&gt;https://github.com/dmi3kno/polite&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Through scraping these sites, I managed to compile a dataset of approximately 5.5 million ratings of about 24.5 thousand beers from roughly 100 thousand users. This data include in-depth reviews along with metadata on both beers (e.g. brewery location, beer style) and users (gender and age, location). I will use an abridged version of this dataset for this post. This data includes only the beer names, user ratings, and unique ids for each user. This is the data I will use to make my recommendation engine. You can download this data here - &lt;a href=&#34;https://duke.box.com/v/bbspring-beer-data&#34; class=&#34;uri&#34;&gt;https://duke.box.com/v/bbspring-beer-data&lt;/a&gt;. Create a Box account to download this data. The source code for this project can be found on my github page (&lt;a href=&#34;https://github.com/datadiarist/&#34; class=&#34;uri&#34;&gt;https://github.com/datadiarist/&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The internet has no shortage of tutorials for coding up recommendation systems in R. Many of these, however, are based on smaller datasets and use ready-made functions for training recommendation models from packages like recommenderlab. In this post, I choose instead to code a recommendation system from scratch. This is necessitated by the size of my dataset; some of the tools in recommenderlab are not meant to accommodate datasets with millions of observations. I also do this to make clear, both to myself and to the reader, the math on which this model is based.&lt;/p&gt;
&lt;p&gt;In the first part of this project, I’m going to import my data as a Spark dataframe and manipulate it using functions from the sparklyr package. Although R has no trouble fitting this dataset (it’s only 292Mb) into the workspace, I find that I can manipulate this data much faster using Spark. I should note that I’m working on a 2018 Macbook Pro with 12 cores. If your computer has fewer cores, you may prefer to import the data as a regular dataframe (use read_csv from the readr package; it’s faster than read.csv). Sparklyr has a dplyr backend, so all my code will work for regular dataframes so long as the dplyr package is loaded. However, this code may take a prohibitively long time to run if you’re working on regular dataframes.&lt;/p&gt;
&lt;p&gt;Let’s begin by importing the data and seeing what it looks like.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;beer_data &amp;lt;- spark_read_csv(sc, &amp;quot;beer_data_fin.csv&amp;quot;)
head(beer_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # Source: spark&amp;lt;?&amp;gt; [?? x 3]
##   beer_full                                user_score user_id
##   &amp;lt;chr&amp;gt;                                         &amp;lt;dbl&amp;gt;   &amp;lt;int&amp;gt;
## 1 Saint Arnold Brewing Company Spring Bock       3.75       1
## 2 (512) Brewing Company (512) White IPA          2.19       2
## 3 Abita Brewing Co. Pecan Ale                    3.99       2
## 4 Anheuser-Busch Bud Light                       1          2
## 5 Anheuser-Busch Budweiser                       2.24       2
## 6 Anheuser-Busch Busch Beer                      1          2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that this Spark dataframe contains three columns. First, beer_full is a character vector with the beer name. To avoid confusing beers with the same name from different breweries, the brewery name precedes the beer name for every element in this vector. Next the user_score column gives the rating on a scale of 1 to 5. Finally, the user_id column shows the id of the user for a given beer rating. Each row represents a beer rating from a given user.&lt;/p&gt;
&lt;p&gt;The first thing I have to do is convert my data into a suitable form for creating a recommendation system. I’m going to use an approach called item-based collaborative filtering, which recommends new products based on similarities between these products and the products the user has rated in the past. Before compute similarities among beers, I first convert my data into a “user-by-beer” matrix. In this matrix, each row will contain all of the ratings from a given user. Because most users have not rated most beers, this will be a very sparse matrix. Given that we have about 100 thousand users and 24.5 thousand beers, the dimensions of this matrix will be about 100,000x24,500 (~ 2.5 billion cells!). Clearly, we will have trouble fitting this data into the workspace as a conventional matrix. Fortunately, the sparsity of the matrix means that we should have no trouble working with the data as a sparse matrix. The Matrix package has tools that will help with this.&lt;/p&gt;
&lt;p&gt;Sparse matrices can be thought of as being made up of three components: the row number (“i”) of a non-empty cell, the column number (“j”) of a non-empty cell, and the value (“x”) in that cell (i.e. the ratings). To create a vector of row numbers for the sparse matrix, I first find the number of ratings associated with each user. I then repeat the user_id the number of times that this user has posted a rating. For example, if user 1 has 3 ratings and user 2 has 4 ratings, the i vector would begin [1, 1, 1, 2, 2, 2, 2]. In other words, the row number for the first 3 entries of the sparse matrix is 1 and the row number of the next 4 entries is 2.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Find number of users in the data 
num_users &amp;lt;- beer_data %&amp;gt;% group_by(user_id) %&amp;gt;% summarise(count = n()) %&amp;gt;%
             sdf_nrow

i &amp;lt;- beer_data %&amp;gt;% 
     # Find number of ratings for each user and sort by user_id
     group_by(user_id) %&amp;gt;% summarise(count = n()) %&amp;gt;% arrange(user_id) %&amp;gt;% 
     # Convert from Spark dataframe to tibble and extract
     # count (number of ratings) vector
     select(count) %&amp;gt;% collect %&amp;gt;% .[[&amp;quot;count&amp;quot;]] 

# Repeat user_id by the number of ratings associated with each user
i &amp;lt;- rep(1:num_users, i)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Producing a vector of columns associated with each user rating is a bit more tricky. First, I create a set of ids for each unique beer in the Spark dataframe. I merge these to the original dataframe, group the dataframe by user_ids, and use the collect_list function to produce a dataframe of beer ids nested in user ids. Finally, I arrange the data by user_id (this part is crucial to making sure the rows and columns match up) and use the explode function to unnest the lists of beer_ids associated with each user. What I’m left with as a vector of column numbers that represent beer ids of user ratings and that are sorted by user id. I’ve annotated the code below to make this as clear as possible. If you have any questions about how this works, please ask them in the comments section.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Creating Spark dataframe with ids for each beer 
beer_key &amp;lt;- beer_data %&amp;gt;% distinct(beer_full) %&amp;gt;% sdf_with_sequential_id

# Merging unique beer ids to the beer data with left_join 
j &amp;lt;- left_join(beer_data, beer_key, by = &amp;quot;beer_full&amp;quot;) %&amp;gt;%
     # Grouping by user_id, nesting beer_ids in user_ids, and sorting by user_id
     group_by(user_id) %&amp;gt;% summarise(user_beers = collect_list(id)) %&amp;gt;%
     arrange(user_id) %&amp;gt;% 
     # Unnesting beer ids (with explode), bringing data into R,
     # and extracting column vector 
     select(user_beers) %&amp;gt;% mutate(vec = explode(user_beers)) %&amp;gt;% select(vec) %&amp;gt;%
     collect %&amp;gt;% .[[&amp;quot;vec&amp;quot;]]

# Turning beer key (beers by unique id) from Spark dataframe to regular dataframe
beer_key &amp;lt;- beer_key %&amp;gt;% collect&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Last, I have to extract a vector of user ratings from the Spark dataframe. To do this I just sort the data by user_id, bring the data into R (with the collect function) and extract the user_score vector.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Sort data by user_id, bring data into R, and extract user_score vector 
x &amp;lt;- beer_data %&amp;gt;% arrange(user_id) %&amp;gt;% select(user_score) %&amp;gt;% collect %&amp;gt;%
     .[[&amp;quot;user_score&amp;quot;]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, I use the sparseMatrix function from the Matrix package to create a sparse matrix.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;beer_sparse &amp;lt;- sparseMatrix(i = i, j = j, x = x)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s how the sparse matrix is represented in R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(beer_sparse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 6 x 24542 sparse Matrix of class &amp;quot;dgCMatrix&amp;quot;
##                                                                        
## [1,] .    .    .    . .   .    .    4.25 . .    .    .    .   .    .   
## [2,] 4.97 3.90 1.00 . .   .    .    .    . .    .    3.82 .   .    .   
## [3,] 4.00 4.00 4.41 3 3.5 3.75 4.25 4.00 4 4.11 3.75 4.00 3.5 3.75 3.86
## [4,] 4.00 4.00 .    . .   .    .    .    . .    .    .    .   .    .   
## [5,] 4.00 2.08 .    . .   3.50 .    4.00 . .    4.45 3.10 .   .    2.52
## [6,] .    .    .    . .   .    .    .    . .    .    3.50 .   .    .   
##                                                                        
## [1,] .    .   .    .    .   .   .    . . .   .   .   .    .    .   .   
## [2,] .    .   .    .    .   .   4.50 . . .   .   3.5 .    .    .   .   
## [3,] 4.25 3.9 3.75 4.25 3.8 4.5 3.94 4 4 4.0 3.5 4.0 4.25 4.25 3.5 3.75
## [4,] .    .   .    .    .   .   .    . . .   .   .   .    .    .   .   
## [5,] .    .   .    4.50 .   4.0 .    . . 3.5 .   .   .    3.50 .   4.00
## [6,] .    .   .    4.00 .   .   .    . . .   .   .   .    .    .   .   
##            
## [1,] ......
## [2,] ......
## [3,] ......
## [4,] ......
## [5,] ......
## [6,] ......
## 
##  .....suppressing columns in show(); maybe adjust &amp;#39;options(max.print= *, width = *)&amp;#39;
##  ..............................&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This provides a snapshot of the first six users in the sparse matrix. The dots represent empty cells, the numbers ratings. While large for a sparse matrix, this object is only 63Mb, which is manageable for our purposes. Our next step is to calculate similarity score among beers. But before we can do this, we need to make some more modifications to the data.&lt;/p&gt;
&lt;p&gt;First, I do a partial singular value decomposition (SVD) of the sparse matrix. An SVD is a kind of matrix factorization that breaks an m by n matrix into three parts: an m by m matrix (“U”), an m by n diagonal matrix (“d”), and an n by n matrix (“V”). A partial SVD keeps only the columns and rows in U and V that correspond with the largest singular values in d. The irlba package in R lets you specify the number of singular values to use in a partial SVD. Most useful for our purposes, irlba is able to perform partial SVDs on sparse matrices.&lt;/p&gt;
&lt;p&gt;In sum, the partial SVD can be thought of as a dimensionality reduction technique that makes performing operations on large matrices less computationally expensive. This is a particularly useful technique for sparse matrices, which can often be transformed into smaller matrices with little loss of information (given that many of the rows are linear combinations of other rows).&lt;/p&gt;
&lt;p&gt;I make the arbitrary choice here to keep the 25 largest singular values, factoring my sparse matrix into three component matrices: an ~105,000x25 matrix (U), a 25x25 matrix (d), and a ~25x24,500 matrix (V). The irlba package automatically transposes V, returning a 24,500x25 matrix. Perhaps in a future post I will consider the implications of how many singular values I use for the validity of the recommender system. For now, here’s the code to perform the partial SVD.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;beer_sparse_svd &amp;lt;- irlba(beer_sparse, 25)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The matrix that interests me is V, which can be thought of as representing ratings patterns for 24,500 beers (the rows) along 25 latent dimensions (the columns), albeit at a loss of some information. Here’s a peak at V.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(beer_sparse_svd$v)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               [,1]         [,2]         [,3]          [,4]         [,5]
## [1,] -0.0256985740 0.0229678228 -0.005193778 -0.0051384220  0.052757404
## [2,] -0.0062777787 0.0049419788 -0.003314623  0.0069633190  0.007472983
## [3,] -0.0005042966 0.0002114207 -0.001387032  0.0003306757  0.001741769
## [4,] -0.0022645231 0.0058033722 -0.001458080  0.0036489648 -0.003678065
## [5,] -0.0296513661 0.0466452519 -0.003826150 -0.0118276998 -0.016366716
## [6,] -0.0105372982 0.0086427265 -0.017151058 -0.0074637989  0.001469116
##              [,6]         [,7]          [,8]          [,9]        [,10]
## [1,] -0.032494474  0.011456998 -0.0108729014  0.0035516823  0.020489015
## [2,] -0.010387374  0.019802582 -0.0009407164 -0.0131732961 -0.002966423
## [3,] -0.001858592  0.002485591  0.0004080029 -0.0024073091 -0.001065007
## [4,]  0.008207708  0.008591826  0.0073319230 -0.0007184202  0.003605576
## [5,]  0.049587403  0.043005553  0.0051627987 -0.0237230351  0.007313664
## [6,] -0.007417017 -0.013712596  0.0028269823 -0.0088606013  0.011820460
##              [,11]        [,12]         [,13]        [,14]         [,15]
## [1,] -0.0155698493 -0.032879244 -0.0113961215  0.039494023 -0.0008142866
## [2,]  0.0222479391 -0.047532155 -0.0116251008 -0.021787547  0.0213485210
## [3,]  0.0027782202 -0.006784769 -0.0022113610 -0.005041011  0.0035659751
## [4,] -0.0016511631 -0.001963106 -0.0003878318 -0.001373413 -0.0033941471
## [5,]  0.0192508819  0.036609922 -0.0007755873  0.022602177 -0.0226693302
## [6,] -0.0009004369 -0.014294341 -0.0004086656  0.006704642 -0.0059271281
##             [,16]         [,17]        [,18]         [,19]         [,20]
## [1,] -0.018455648  0.0198518744 -0.025979319 -0.0040638017 -2.509825e-02
## [2,]  0.024394173  0.0173652077  0.034726167 -0.0359433350 -3.948808e-03
## [3,]  0.004901526  0.0010760466  0.006260497 -0.0042772709 -4.698748e-05
## [4,]  0.010084600 -0.0001870042 -0.002243230 -0.0023221599  5.838162e-03
## [5,]  0.001787687  0.0110909919  0.021767510  0.0355030003 -2.935001e-02
## [6,] -0.015706574 -0.0138034722  0.006490726 -0.0006405754 -3.582702e-03
##              [,21]        [,22]        [,23]        [,24]        [,25]
## [1,]  0.0025110953 -0.011861829 -0.004598431 -0.011089507  0.003169127
## [2,]  0.0007522815  0.023399755  0.002494241  0.005222926 -0.021329421
## [3,]  0.0001123453  0.004374985 -0.000861592  0.003446859 -0.001396566
## [4,]  0.0005898593 -0.008568204 -0.006001832  0.002906891  0.004491294
## [5,]  0.0095077194  0.059850068  0.072044630  0.054602812  0.091765192
## [6,] -0.0060200612 -0.007056966 -0.016547627  0.001281847  0.011645727&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This displays numbers that place 6 beers in a 25-dimensional space. These numbers don’t mean anything prima facie. They are both positive and negative and cluster very closely to 0. Hopefully, calculating similarity scores among these 25-dimensional vectors will produce results that make sense. I am now ready to calculate similarity scores among my ~24,500 beers. While there are many options out there for measuring similarity between vectors, I choose one of the simplest and most commonly-used ones: cosine distance. This will likely not produce the best possible recommendation engine from this data; I choose this measure to limit the length of this post and because it’s suitable for a demonstration of how to code a recommendation system on big-ish data from scratch in R. The cosine distance between two vectors is simply the dot product of the vectors divided by the product of the norms of these vectors. When these vectors are the same this distance will be 1. Otherwise, we will get a number between 0 and 1. I calculate the cosine distance with a function from the lsa package.&lt;/p&gt;
&lt;p&gt;Before we can find similarity scores among the beers in the data, we must consider one last issue. Calculating similarity scores among 24,500 beers would produce 24,500 choose 2, or roughly 300 million, similarity scores. We once again find ourselves exceeding the size limits of what can be stored in the R workspace. I sidestep this issue by computing all similarity scores for each beer and only keeping the largest 500. 500 is another arbitrary cutoff and in a future post I may examine how varying this cutoff affects the results.&lt;/p&gt;
&lt;p&gt;Although the 500 cutoff resolves the size concern, we are still left with the task of computing 300 million similarity scores, most of which will be discarded. To deal with this, I use the foreach, parallel, and doParallel packages to utilize the 12 cores of my computer for this task.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Setting up and registering a cluster for parallel processing
cl &amp;lt;- makeCluster(detectCores() - 1)
registerDoParallel(cl)

# Setting up the foreach loop and pre-loading packages used within the loop
item_similarity_matrix &amp;lt;- foreach(i = 1:nrow(beer_key),
                             .packages = c(&amp;quot;dplyr&amp;quot;, &amp;quot;Matrix&amp;quot;, &amp;quot;lsa&amp;quot;)) %dopar% {
  
  # Calculating the cosine distances between a given beer (i) and all the
  # beers in the sparse matrix
  sims &amp;lt;- cosine(t(beer_sparse_svd$v)[,i], t(beer_sparse_svd$v))   
  
  # Finding arrange the cosine distances in descending order, 
  # finding the 501th biggest one
  cutoff &amp;lt;- sims %&amp;gt;% tibble %&amp;gt;% arrange(desc(.)) %&amp;gt;% .[501,] %&amp;gt;% .[[&amp;quot;.&amp;quot;]]
  
  # Limiting the beer_key dataframe to beers with large enough
  # similarity scores
  sims.test &amp;lt;- beer_key %&amp;gt;% .[which(sims &amp;gt;= cutoff &amp;amp; sims &amp;lt; 1),]
  
  # Appending similarity scores to the abridged dataframe and sorting by
  # similarity score
  sims.test &amp;lt;- sims.test %&amp;gt;% mutate(score = sims[sims &amp;gt;= cutoff &amp;amp; sims &amp;lt; 1]) %&amp;gt;%
    arrange(desc(score))
  
  # Changing column names of the final tibble
  names(sims.test) &amp;lt;- c(beer_key[i,] %&amp;gt;% .[[&amp;quot;beer_full&amp;quot;]], &amp;quot;id&amp;quot;, &amp;quot;score&amp;quot;) 
  
  return(sims.test)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On my computer, this code took about fifteen minutes to run. On a computer with fewer cores, this will take a lot longer. This code returns a list of ~24,500 tibbles, each with 500 rows. This is a large object, 1.4Gb. If this affects R’s performance we can always return to this bit of code and change 500 to a smaller number.&lt;/p&gt;
&lt;p&gt;Let’s have a look at a tibble from this list. I’ll search for one of my favorite beers, Ballast Point Sculpin, to find out which beers are most similar to the Sculpin.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Searching for Sculpin in the beer_key
grep(&amp;quot;Sculpin&amp;quot;, beer_key$beer_full, value = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Ballast Point Brewing Company Sculpin - Habanero&amp;quot;  
## [2] &amp;quot;Ballast Point Brewing Company Sculpin - Unfiltered&amp;quot;
## [3] &amp;quot;Ballast Point Brewing Company Sculpin - Grapefruit&amp;quot;
## [4] &amp;quot;Ballast Point Brewing Company Sculpin - Spruce Tip&amp;quot;
## [5] &amp;quot;Ballast Point Brewing Company Sculpin - Aloha&amp;quot;     
## [6] &amp;quot;Ballast Point Brewing Company Sculpin&amp;quot;             
## [7] &amp;quot;Ballast Point Brewing Company Sculpin - Pineapple&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, many kinds of Sculpins appear in the dataset. I’ll index the list (called ‘item_similarity_matrix’) for the original Sculpin.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Beers similar to Sculpin 
item_similarity_matrix[grep(&amp;quot;Sculpin&amp;quot;, beer_key$beer_full)[6]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## # A tibble: 500 x 3
##    `Ballast Point Brewing Company Sculpin`                   id score
##    &amp;lt;chr&amp;gt;                                                  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1 Lagunitas Brewing Company Lagunitas Sucks              22503 0.698
##  2 Stone Brewing Enjoy By IPA                              6125 0.688
##  3 Firestone Walker Brewing Co. Union Jack IPA            20412 0.595
##  4 Lagunitas Brewing Company Lagunitas IPA                18369 0.579
##  5 Russian River Brewing Company Pliny The Elder          12336 0.570
##  6 Cigar City Brewing Jai Alai IPA                           12 0.541
##  7 Green Flash Brewing Co. West Coast IPA                  4096 0.526
##  8 Lagunitas Brewing Company A Little Sumpin&amp;#39; Sumpin&amp;#39; Ale  2106 0.523
##  9 Bear Republic Brewing Co. Racer 5 India Pale Ale        6211 0.519
## 10 Lagunitas Brewing Company Hop Stoopid                   2022 0.506
## # … with 490 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This tibble displays the 500 most similar beers to Sculpin sorted in decreasing order of their similarity. In my opinion as an inveterate beer drinker, this list has face validity. The list consists mainly of American IPAs, which is what Sculpin is. I can attest to the similarity of some of the beers on this list (Bear Republic’s Racer 5, Stone’s Enjoy By) to Sculpin. It’s also worth noting that many of the beers on this list come from Californian breweries, which is where Sculpin is brewed. This likely reflects a tendency of reviewers to be more familiar with beers in their own region. This sort of regional clustering presents problems for the validity of the recommendation system, especially if one’s region has no bearing on one’s beer preferences. Product ratings are not randomly distributed across users and therefore beer similarity scores are partly based on factors that are not related to qualities of the beers themselves.&lt;/p&gt;
&lt;p&gt;Still, I’m encouraged by the list of beers similar to Sculpin. In fact, these are some of my favorite beers. Just to show that I’m not cherry picking, I’ll randomly select a beer from my list of beers and check its similarity scores for face validity.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)
item_similarity_matrix[base::sample(nrow(beer_key), 1)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## # A tibble: 500 x 3
##    `Pipeworks Brewing Company Citra Saison`                      id score
##    &amp;lt;chr&amp;gt;                                                      &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1 Pipeworks Brewing Company Just Drink It, Dummy!            23014 0.978
##  2 Pipeworks Brewing Company Amarillo                           320 0.973
##  3 Pipeworks Brewing Company Fully Hoperational Battlestation  8674 0.964
##  4 Pipeworks Brewing Company Nelson Sauvin                     7106 0.961
##  5 Pipeworks Brewing Company Mosaic                           19037 0.958
##  6 Spiteful Brewing The Whale Tickler Mango IPA                 973 0.956
##  7 BrickStone Restaurant &amp;amp; Brewery HopSkipNImDrunk            12548 0.954
##  8 Pipeworks Brewing Company Derketo                           6267 0.953
##  9 Pipeworks Brewing Company Kwingston&amp;#39;s Kitty Cat-ina         2548 0.949
## 10 Pipeworks Brewing Company Beejay&amp;#39;s Weirdo Brown Ale        22642 0.949
## # … with 490 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we have a Saison from a smaller Chicago-based brewery. Given that reviewers of this beer were way more likely to review other beers from Pipeworks, most of the top beers on this list are Pipeworks beers. The two non-Pipeworks beers in the top ten come from other breweries based in Illinois. Again, this is a common issue with recommendation systems. For instance, I’ve been recommended movies on Netflix that have cast members in common with movies I’ve rated highly. Some people like receiving recommendations for movies with their favorite actors, just as some people may wish to receive recommendations for other beers from their favorite breweries. However, for me an ideal recommendation system would estimate similarity by the most intrinsic features of a product (in our case, beer taste). It may be fair to conclude from these preliminary validity checks that this recommendation system will work better for beers from more established breweries that distribute beers on a national scale. For these beers, patterns in user ratings are more likely to be based on beer taste and less likely to be based on brewery and region.&lt;/p&gt;
&lt;p&gt;For the last part of this post, I will write a function that takes a 24542 (the number of beers) length vector and returns a list of beer recommendations. This vector will have 0’s for beers that the user has not rated and values between 1 and 5 for beers that the user has rated. Let’s return to our sparse matrix and sample a user who has reviewed a lot of beers. I happen to know that the user with id 3 has rated a few thousand beers, so we’ll use this user as our example user.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Creating a 24542-length vector of beer ratings for user 3
example_user &amp;lt;- beer_sparse[3,]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To predict beer ratings based on a user’s past ratings, I use the following formula:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://datadiarist.github.io/post/2019-07-16-building-a-recommendation-system-with-beer-data_files/Screen%20Shot%202019-07-19%20at%205.05.07%20PM.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;Following this equation, the predicted rating of a given beer is just the average of the ratings of similar beers weighted by their similarity scores. My function will recommend a set of beers based on which beers have the highest predicted ratings according to this equation.&lt;/p&gt;
&lt;p&gt;The first thing that I need to is find the beer ids of the beers that the user has already rated. This is easy - I just search the vector for non-zero elements.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rated_beer_ids &amp;lt;- which(example_user != 0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, I pull the similarity scores of all beers that are similar to at least one beer that the user has rated. For each of these beers I only extract similarity scores between the beer and the rated beers. I use the map function from the purrr package to do this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim_scores &amp;lt;- map(item_similarity_matrix, ~.x %&amp;gt;%
                    filter(id %in% rated_beer_ids) %&amp;gt;%
                    .[[&amp;quot;score&amp;quot;]])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now I want to create a vector with candidate beers to recommend to the user. These are beers that are similar to at least 5 of the beers that the user has rated. Why 5 and not 1? My thinking here is that my equation would give beers with one similar beer to a rated beer a predicted rating of that beer’s rating. I am more confidence in a predicted rating that is based on a weighted average of several rated beers, rather than one or a few weighted beers. I may adjust this parameter in the future to see how it affects model validity.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;candidate_beer_ids &amp;lt;- which(sim_scores %&amp;gt;% map(., ~length(.x) &amp;gt;= 5) %&amp;gt;% unlist)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This vector, candidate_beer_ids, gives the positions of beers that have at least 10 similar beers to beers the example user has rated. It is likely that some of these include beers the user has rated. We don’t want to predict beer ratings for rated beers, so I filter these.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;candidate_beer_ids &amp;lt;- candidate_beer_ids[!(candidate_beer_ids %in%
                                             rated_beer_ids)]

# Number of candidate beers 
length(candidate_beer_ids)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 19149&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I am now ready to compute predicted ratings for the candidate beers (i.e. beers that have some degree of similarity to at least 5 beers that the user has rated). I start by calculating the denominators of these predicted ratings. For each candidate beer, is this the sum of similarity scores to beers the user has rated.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;denoms &amp;lt;- map(item_similarity_matrix[candidate_beer_ids], ~.x %&amp;gt;%
                filter(id %in% rated_beer_ids) %&amp;gt;% .[[&amp;quot;score&amp;quot;]] %&amp;gt;% sum)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The numerators of these predicted ratings are the products of similarity scores and ratings of beers the user has rated. I use the map function to create two lists - one of vectors of similarity scores for each candidate beer and one of vectors of ratings of similar beers to each candidate beer. Finally, I use purrr’s map2 function, which takes two lists as inputs, and take sums of the dot products of these lists of vectors. The resulting list contains the numerators of predicted ratings for each candidate beer.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# List of similarity scores 
sims_vecs &amp;lt;- map(item_similarity_matrix[candidate_beer_ids],
                 ~.x %&amp;gt;% filter(id %in% rated_beer_ids) %&amp;gt;% .[[&amp;quot;score&amp;quot;]])

# List of ratings 
ratings_vecs &amp;lt;- map(item_similarity_matrix[candidate_beer_ids], 
                     ~example_user[.x %&amp;gt;% filter(id %in% rated_beer_ids) %&amp;gt;%
                                     .[[&amp;quot;id&amp;quot;]]])

nums &amp;lt;- map2(sims_vecs, ratings_vecs, ~sum(.x*.y))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The last step is to divide each element in the numerators list by its corresponding denominator from the denominators list to get predicted ratings. I once again use the map2 function for this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predicted_ratings &amp;lt;- map2(nums, denoms, ~.x/.y)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that I have a list of predicted ratings for candidate beers I can create a tibble that displays beer names and their predicted ratings. I sort this in descending order of predicted rating and sample the first few rows to see which beers my recommendation system would recommend this user.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pred_ratings_tbl &amp;lt;- tibble(beer_full = beer_key %&amp;gt;% 
                          filter(id %in% candidate_beer_ids) %&amp;gt;% .[[&amp;quot;beer_full&amp;quot;]], 
                          pred_rating = predicted_ratings %&amp;gt;% unlist) %&amp;gt;%
                          arrange(desc(pred_rating))
head(pred_ratings_tbl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 2
##   beer_full                              pred_rating
##   &amp;lt;chr&amp;gt;                                        &amp;lt;dbl&amp;gt;
## 1 Frost Beer Works Hush Hush                    4.29
## 2 Sly Fox Brewing Company Valor                 4.27
## 3 Mason&amp;#39;s Brewing Company Liquid Rapture        4.27
## 4 Benchtop Brewing Company Proven Theory        4.25
## 5 Highland Brewing Daycation                    4.25
## 6 SingleCut Beersmiths KT66 IPL                 4.24&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The top-six recommended beers for this user include 2 American Imperial IPAs, 2 American IPAs, a Belgian pale ale, and an IPL (India Pale Lager). Apparently the user has a preference for IPAs. These beers are geographically distributed, although many of them are in New England. I could check if the user is from New England; this information is in the full data. Now let’s check the face validity of these recommendations. I’m going to pull a list of some of the top beers that this user has rated.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tibble(beer_full = beer_key[which(example_user != 0),] %&amp;gt;% .[[&amp;quot;beer_full&amp;quot;]], 
       rating = example_user[which(example_user != 0)]) %&amp;gt;%
       arrange(desc(rating)) %&amp;gt;% head&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 2
##   beer_full                                              rating
##   &amp;lt;chr&amp;gt;                                                   &amp;lt;dbl&amp;gt;
## 1 Wicked Weed Brewing Freak Of Nature                         5
## 2 SingleCut Beersmiths Jenny Said Double Dry-Hopped IIPA      5
## 3 Fremont Brewing Company Coconut B-Bomb                      5
## 4 Roscoe&amp;#39;s Hop House Pale Ale                                 5
## 5 Firestone Walker Brewing Co. Double Double Barrel Ale       5
## 6 Green Flash Brewing Co. Spanish Trampoline                  5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These ratings make sense given the user’s recommended beers. Many of these beers are American IPAs, and some of them are American Imperial IPAs. They also come from a geographically-distributed set of breweries. We don’t see any sign of clustering in New England from this short list; that the recommended beers are disproportionately from New England could be a sign that the highest rated IPA are from New England, or it could be a coincidence. Of course we could test this by looking at a longer list of this user’s favorite beers.&lt;/p&gt;
&lt;p&gt;Instead, I encourage you to try this recommendation system on your own set of beer ratings. The function below contains the code for computing predicted beer ratings. Enter a vector of beer ratings and the function will return six recommendations. To create a vector of beer ratings, first create a 24542-length vector of 0’s. Then, search beer_key to find the position of the beer you want to rate. I did this with Sculpin above. Enter your rating in the vector of 0’s at the appropriate position. When you’ve created your vector of beer ratings, enter it into the function. One last thing - while the recommendation system discussed above only recommended beers with at least 5 similar beers to beers that the user rated, this function has a default of 3 similarity beers. This is the second argument of the function, similarity_cutoff. To change this default, simply enter a number other than 3 as the default.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;recommend_beers &amp;lt;- function(input_vec, similarity_cutoff = 3){
  
# Replace missing values with 0
input_vec[is.na(input_vec)] &amp;lt;- 0 
  
if(length(input_vec) != nrow(beer_key)){
  stop(&amp;quot;Please enter a 24502-length vector!&amp;quot;)}else if(
  length(input_vec[input_vec &amp;gt; 5 | input_vec &amp;lt; 0]) &amp;gt; 0){
    stop(&amp;quot;Vector can only contain values between 0 and 5!&amp;quot;)}
  
## ------------------------------------------------------------------------
rated_beer_ids &amp;lt;- which(input_vec != 0)

## ------------------------------------------------------------------------
sim_scores &amp;lt;- map(item_similarity_matrix, ~.x %&amp;gt;%
                    filter(id %in% rated_beer_ids) %&amp;gt;%
                    .[[&amp;quot;score&amp;quot;]])

## ------------------------------------------------------------------------
candidate_beer_ids &amp;lt;- which(sim_scores %&amp;gt;% 
                              map(., ~length(.x) &amp;gt;= similarity_cutoff) %&amp;gt;%
                              unlist)

if(!is_empty(candidate_beer_ids)){

## ------------------------------------------------------------------------
candidate_beer_ids &amp;lt;- candidate_beer_ids[!(candidate_beer_ids %in%
                                             rated_beer_ids)]

## ------------------------------------------------------------------------
denoms &amp;lt;- map(item_similarity_matrix[candidate_beer_ids], ~.x %&amp;gt;%
                filter(id %in% rated_beer_ids) %&amp;gt;% .[[&amp;quot;score&amp;quot;]] %&amp;gt;% sum)

## ------------------------------------------------------------------------
# List of similarity scores 
sims_vecs &amp;lt;- map(item_similarity_matrix[candidate_beer_ids],
                 ~.x %&amp;gt;% filter(id %in% rated_beer_ids) %&amp;gt;% .[[&amp;quot;score&amp;quot;]])

# List of ratings 
ratings_vecs &amp;lt;- map(item_similarity_matrix[candidate_beer_ids], 
                    ~input_vec[.x %&amp;gt;% filter(id %in% rated_beer_ids) %&amp;gt;%
                                    .[[&amp;quot;id&amp;quot;]]])

nums &amp;lt;- map2(sims_vecs, ratings_vecs, ~sum(.x*.y))

## ------------------------------------------------------------------------
predicted_ratings &amp;lt;- map2(nums, denoms, ~.x/.y)

## ------------------------------------------------------------------------
pred_ratings_tbl &amp;lt;- tibble(beer_full = beer_key %&amp;gt;% 
                             filter(id %in% candidate_beer_ids) %&amp;gt;%
                             .[[&amp;quot;beer_full&amp;quot;]], 
                           pred_rating = predicted_ratings %&amp;gt;% unlist) %&amp;gt;%
                           arrange(desc(pred_rating))

head(pred_ratings_tbl) %&amp;gt;% return}else{
  print(&amp;quot;You haven&amp;#39;t rated enough beers!&amp;quot;)}
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s test the function on a random user from the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)
recommend_beers(beer_sparse[base::sample(num_users, 1),])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 2
##   beer_full                                                     pred_rating
##   &amp;lt;chr&amp;gt;                                                               &amp;lt;dbl&amp;gt;
## 1 Boston Beer Company (Samuel Adams) Harvest Saison                    4.53
## 2 Coors Brewing Company (Molson-Coors) Blue Moon Short Straw F…        4.46
## 3 Anheuser-Busch Shock Top Honey Bourbon Cask Wheat                    4.45
## 4 Coors Brewing Company (Molson-Coors) Blue Moon Valencia Ambe…        4.44
## 5 Coors Brewing Company (Molson-Coors) Blue Moon Farmhouse Red…        4.42
## 6 Coors Brewing Company (Molson-Coors) Blue Moon Caramel Apple…        4.41&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It worked!&lt;/p&gt;
&lt;p&gt;If you don’t want to bother running this code on your computer, I’ve created an app that produces recommendations and updates them as you add beer ratings. Navigate over to the Shiny section of my website to check it out, and thanks for reading!&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
